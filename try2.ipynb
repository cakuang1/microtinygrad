{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "def drelu(x):\n",
    "    return np.where(x > 0, 1,0)\n",
    "def mse(t,p):\n",
    "    \"\"\"\n",
    "    Returns mean sqaured error given two arrays of the same length\n",
    "    \"\"\"\n",
    "    return sum((t-p)**2)/len(t)\n",
    "\n",
    "def softmax(x):\n",
    "    # assumes x is a vector\n",
    "\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden = []) -> None:\n",
    "        self.sizes = hidden\n",
    "        self.biases = None\n",
    "        self.weights = None\n",
    "        self.type = None\n",
    "        self.map = {}\n",
    "    # detects what type of model we want and constructs our bias,weights,type and map dependent on the dataframe and series given\n",
    "    def construct(self,X,Y):\n",
    "        inputsize = X.shape[1]\n",
    "        self.sizes.insert(0,inputsize)\n",
    "        unique_values = len(Y.unique())\n",
    "        self.sizes.append(unique_values)\n",
    "        self.type = 'classification'\n",
    "        self.map = {index: value for index, value in enumerate(Y.unique())}\n",
    "        self.biases = [np.zeros((y, 1)) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) * np.sqrt(2/x) for x, y in zip(self.sizes[:-1],self.sizes[1:])]\n",
    "        self.trainacc = []\n",
    "        self.testacc = []\n",
    "    # Helper for train function\n",
    "    # Feeds our input into our neural network and determines the loss \n",
    "    def forward(self,input,answers):\n",
    "        ## Input is supposed to be the training matrix\n",
    "        input = input.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            dot = np.dot(w, input)+b\n",
    "            if np.array_equal(self.biases[len(self.biases) - 1] ,b):\n",
    "                if self.type == 'classification':\n",
    "                    input = softmax(dot.T)\n",
    "            else:\n",
    "                input = relu(dot)\n",
    "        # input is now the output activation\n",
    "        # If the type is regression, then the length of the output activation should be 1. \n",
    "        if self.type == 'classification':\n",
    "            ## Accuracy instead of loss \n",
    "            currentsum = 0\n",
    "            for row,index in zip(input,answers):\n",
    "                if np.argmax(row) == index:\n",
    "                    currentsum += 1\n",
    "            loss = currentsum\n",
    "        return  loss\n",
    "    \n",
    "    # Returns a tuple of dw,db, which are layer by layer arrays that represent partials\n",
    "    def backprop(self,inputx,inputy):\n",
    "        # input x is a 1d vector ,input y is a scalar\n",
    "        partialb = [np.zeros(b.shape) for b in self.biases]\n",
    "        partialw = [np.zeros(w.shape) for w in self.weights]\n",
    "        storedsums = []\n",
    "        input = inputx.reshape(inputx.size,1)\n",
    "        storedactivations = [input.T]\n",
    "        # Forward propagation, storing the su\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            dot = np.dot(w, input)+b\n",
    "            storedsums.append(dot)\n",
    "            ## If we are on the last iteration, then our activation function is linear if its regression, or softmax if its classification\n",
    "            if np.array_equal(self.biases[len(self.biases) - 1] ,b):\n",
    "                if self.type == 'classification':\n",
    "                    ## Ensure no overflow\n",
    "                    input = softmax(dot.T)\n",
    "            else:\n",
    "                input = relu(dot)\n",
    "            storedactivations.append(input.T)\n",
    "        ## Actual backpropagation\n",
    "        #First Layer\n",
    "        ## Regression case. Thus, the activation for our output for our output layer is linear. Our loss function is mean square error, thus delta (dL/dz) is -2(y-z).\n",
    "        ## Classification case. Delta of the output layer is dependent on the output layer. Credit to https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1 for doing the dirty work\n",
    "        if self.type == 'classification':\n",
    "            dz = storedactivations[-1]\n",
    "            dz[inputy] = dz[inputy] - 1\n",
    "            assert dz.shape == partialb[-1].shape\n",
    "            partialb[-1] = dz\n",
    "            dw = np.dot(dz, storedactivations[-2])\n",
    "            assert dw.shape == partialw[-1].shape\n",
    "            partialw[-1]= dw\n",
    "        \n",
    "        # Finding partials for the rest of the layers\n",
    "        for i in range(2,len(self.sizes)): \n",
    "            dz = drelu(storedsums[-i])\n",
    "\n",
    "            delta = np.dot(self.weights[-i + 1].T,partialb[-i + 1]) * dz\n",
    "            assert delta.shape == partialb[-i].shape\n",
    "            partialb[-i] = delta   \n",
    "            weight = np.dot(delta, storedactivations[-i - 1])\n",
    "            assert weight.shape == partialw[-i].shape\n",
    "            partialw[-i] = weight\n",
    "        return (partialw,partialb)\n",
    "\n",
    "    # Updates our weights and biases with a given batch\n",
    "    def updatebatch(self,batchx,batchy,learningrate = 0.01):\n",
    "        ## Assumptions these are numpy arrays\n",
    "\n",
    "        currentsumb = [np.zeros(b.shape) for b in self.biases]\n",
    "        currentsumw = [np.zeros(w.shape) for w in self.weights]\n",
    "        batchx = batchx.values\n",
    "\n",
    "        for x,y in zip(batchx,batchy):\n",
    "            gradientw,gradientb = self.backprop(x,y)\n",
    "            for i in range(len(currentsumb)):\n",
    "                currentsumb[i] = (gradientb[i] + currentsumb[i])\n",
    "                currentsumw[i] = (gradientw[i] + currentsumw[i])\n",
    "        \n",
    "        for i in range(len(currentsumb)):\n",
    "            self.weights[i] = self.weights[i] - (learningrate/len(batchy)) * currentsumw[i]\n",
    "            self.biases[i] = self.biases[i] - (learningrate/len(batchy)) * currentsumb[i]\n",
    "    \n",
    "\n",
    "            \n",
    "    def train(self,X,Y,batchsize = 32,epoch = 10,testx = None,testy = None):\n",
    "        assert isinstance(X, pd.DataFrame) \n",
    "        assert isinstance(Y,pd.Series)\n",
    "        # Constructs the array for training and test error\n",
    "        self.construct(X,Y)\n",
    "        if testx and testy:\n",
    "            self.testacc.append(self.forward(testx,testy))\n",
    "        self.testacc.append(self.forward(X,Y))\n",
    "        if self.type == 'classification':\n",
    "            Y = pd.factorize(Y)[0]\n",
    "        minibatchesX = np.array_split(X, len(X) // batchsize)\n",
    "        minibatchesY = np.array_split(Y, len(Y) // batchsize)\n",
    "        #split dataframe into batches of size batchsize\n",
    "        for i in range(epoch):\n",
    "            for x,y in zip(minibatchesX,minibatchesY):\n",
    "                self.updatebatch(x,y)\n",
    "            \n",
    "            if self.type == 'classification':\n",
    "                trainaccuracy = self.forward(X,Y)\n",
    "                self.trainacc.append(trainaccuracy)\n",
    "                print('Your training accuracy after epoch ' + str(i + 1) + ' is ' + str(trainaccuracy))\n",
    "                if testx and testy:\n",
    "                    testaccuracy = self.forward(testx,testy)\n",
    "                    print('Your test accuracy after epoch ' + str(i + 1) + ' is ' + str(testaccuracy))\n",
    "                \n",
    "    def graph(self):\n",
    "        plt.xlabel('X-axis')\n",
    "        plt.ylabel('Y-axis')\n",
    "        plt.title('Accuracy/Epoch')\n",
    "        if len(self.trainacc) == len(self.testacc):\n",
    "            \n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        given some data frame X, return the predicted labels\n",
    "        \"\"\"\n",
    "        e\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or1\n",
    "trainx = pd.DataFrame(X)\n",
    "trainy = pd.Series(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "# Create a Pandas DataFrame from the data and target variables\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty =  df['target']\n",
    "tx = df.drop('target',axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\n",
      "Your training accuracy after epoch 1 is 50\n",
      "Your training accuracy after epoch 2 is 65\n",
      "Your training accuracy after epoch 3 is 50\n",
      "Your training accuracy after epoch 4 is 50\n",
      "Your training accuracy after epoch 5 is 100\n",
      "Your training accuracy after epoch 6 is 100\n",
      "Your training accuracy after epoch 7 is 100\n",
      "Your training accuracy after epoch 8 is 100\n",
      "Your training accuracy after epoch 9 is 100\n",
      "Your training accuracy after epoch 10 is 100\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([30,20,30])\n",
    "nn.train(tx,ty)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -1\n",
       "1    -1\n",
       "2    -1\n",
       "3     1\n",
       "4    -1\n",
       "     ..\n",
       "95    1\n",
       "96   -1\n",
       "97    1\n",
       "98    1\n",
       "99    1\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
