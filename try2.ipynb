{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('house.csv')\n",
    "dataset.drop(['Id'],\n",
    "             axis=1,\n",
    "             inplace=True)\n",
    "\n",
    "dataset['SalePrice'] = dataset['SalePrice'].fillna(\n",
    "  dataset['SalePrice'].mean())\n",
    "\n",
    "\n",
    "new_dataset = dataset.dropna()\n",
    "s = (new_dataset.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "OH_encoder = OneHotEncoder(sparse_output=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(new_dataset[object_cols]))\n",
    "OH_cols.index = new_dataset.index\n",
    "OH_cols.columns = OH_encoder.get_feature_names_out()\n",
    "df_final = new_dataset.drop(object_cols, axis=1)\n",
    "df_final = pd.concat([df_final, OH_cols], axis=1)\n",
    "\n",
    "X = df_final.drop('SalePrice', axis=1)  # Specify the column name for the target variable\n",
    "y = df_final['SalePrice']  # Specify the column name for the target variable\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('house.csv')\n",
    "dataset['BldgType'].dtype == 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.866984</td>\n",
       "      <td>0.004605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.113226</td>\n",
       "      <td>-0.553740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.643369</td>\n",
       "      <td>-0.561538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.731905</td>\n",
       "      <td>-0.360165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.069752</td>\n",
       "      <td>1.010440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.621144</td>\n",
       "      <td>-0.406344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.640603</td>\n",
       "      <td>0.675796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.047312</td>\n",
       "      <td>0.879076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.178699</td>\n",
       "      <td>-0.652375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.355684</td>\n",
       "      <td>-0.500851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1\n",
       "0   1.866984  0.004605\n",
       "1   1.113226 -0.553740\n",
       "2   0.643369 -0.561538\n",
       "3   1.731905 -0.360165\n",
       "4  -0.069752  1.010440\n",
       "..       ...       ...\n",
       "95  0.621144 -0.406344\n",
       "96 -0.640603  0.675796\n",
       "97  0.047312  0.879076\n",
       "98  1.178699 -0.652375\n",
       "99  1.355684 -0.500851\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "def drelu(x):\n",
    "    return np.where(x > 0, 1,0)\n",
    "def mse(t,p):\n",
    "    \"\"\"\n",
    "    Returns mean sqaured error given two arrays of the same length\n",
    "    \"\"\"\n",
    "    return sum((t - p) ** 2 for t, p in zip(t, p)) / len(t)\n",
    "\n",
    "def softmax(x):\n",
    "    # assumes x is a vector\n",
    "\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden = []) -> None:\n",
    "        self.sizes = hidden\n",
    "        self.biases = None\n",
    "        self.weights = None\n",
    "        self.type = None\n",
    "        self.map = {}\n",
    "    # detects what type of model we want and constructs our bias,weights,type and map dependent on the dataframe and series given\n",
    "    def construct(self,X,Y):\n",
    "        inputsize = X.shape[1]\n",
    "        self.sizes.insert(0,inputsize)\n",
    "        # Checking if its categorical or regression NN\n",
    "        if Y.dtype == 'float64':\n",
    "            self.sizes.append(1)\n",
    "            self.type = 'regression'\n",
    "        else:\n",
    "            unique_values = len(Y.unique())\n",
    "            self.sizes.append(unique_values)\n",
    "            self.type = 'classification'\n",
    "            self.map = {index: value for index, value in enumerate(Y.unique())}\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1],self.sizes[1:])]\n",
    "\n",
    "    # Helper for train function\n",
    "    # Feeds our input into our neural network and determines the loss \n",
    "    def forward(self,input,answers):\n",
    "        ## Input is supposed to be the training matrix\n",
    "        input = input.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            dot = np.dot(w, input)+b\n",
    "            if np.array_equal(self.biases[len(self.biases) - 1] ,b):\n",
    "                if self.type == 'regression':\n",
    "                    input = dot\n",
    "                elif self.type == 'classification':\n",
    "                    input = softmax(dot.T)\n",
    "            else:\n",
    "                input = relu(dot)\n",
    "        # input is now the output activation\n",
    "        # If the type is regression, then the length of the output activation should be 1. \n",
    "        if self.type == 'regression':\n",
    "            loss = mse(input,answers)\n",
    "        elif self.type == 'classification':\n",
    "            ## Accuracy instead of loss \n",
    "            currentsum = 0\n",
    "            for row,index in zip(input,answers):\n",
    "                if np.argmax(row) == index:\n",
    "                    currentsum += 1\n",
    "            loss = currentsum\n",
    "        return  loss\n",
    "    \n",
    "    # Returns a tuple of dw,db, which are layer by layer arrays that represent partials\n",
    "    def backprop(self,inputx,inputy):\n",
    "        # input x is a 1d vector ,input y is a scalar\n",
    "        partialb = [np.zeros(b.shape) for b in self.biases]\n",
    "        partialw = [np.zeros(w.shape) for w in self.weights]\n",
    "        storedsums = []\n",
    "        input = inputx.reshape(inputx.size,1)\n",
    "        storedactivations = [input.T]\n",
    "        # Forward propagation, storing the su\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            dot = np.dot(w, input)+b\n",
    "            storedsums.append(dot)\n",
    "            ## If we are on the last iteration, then our activation function is linear if its regression, or softmax if its classification\n",
    "            if np.array_equal(self.biases[len(self.biases) - 1] ,b):\n",
    "                if self.type == 'regression':\n",
    "                    input = dot\n",
    "                elif self.type == 'classification':\n",
    "                    ## Ensure no overflow\n",
    "                    input = softmax(dot.T)\n",
    "            else:\n",
    "                input = relu(dot)\n",
    "            storedactivations.append(input.T)\n",
    "        ## Actual backpropagation\n",
    "        #First Layer\n",
    "        ## Regression case. Thus, the activation for our output for our output layer is linear. Our loss function is mean square error, thus delta (dL/dz) is -2(y-z).\n",
    "        if self.type == 'regression':\n",
    "            dz = -2 * (y - storedactivations[-1])\n",
    "            assert dz.shape == partialb[-1].shape\n",
    "            partialb[-1] = dz \n",
    "            dw = np.dot(dz, storedactivations[-2].T)\n",
    "            assert dw.shape == partialw[-1].shape\n",
    "            partialw[-1]= dw\n",
    "        ## Classification case. Delta of the output layer is dependent on the output layer. Credit to https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1 for doing the dirty work\n",
    "        elif self.type == 'classification':\n",
    "            dz = storedactivations[-1]\n",
    "            dz[inputy] = dz[inputy] - 1\n",
    "            assert dz.shape == partialb[-1].shape\n",
    "            partialb[-1] = dz\n",
    "            dw = np.dot(dz, storedactivations[-2])\n",
    "            assert dw.shape == partialw[-1].shape\n",
    "            partialw[-1]= dw\n",
    "        \n",
    "        # Finding partials for the rest of the layers\n",
    "        for i in range(2,len(self.sizes)): \n",
    "            dz = drelu(storedsums[-i])\n",
    "            delta = np.dot(self.weights[-i + 1].T,partialb[-i + 1]) * dz\n",
    "            assert delta.shape == partialb[-i].shape\n",
    "            partialb[-i] = delta   \n",
    "            weight = np.dot(delta, storedactivations[-i - 1])\n",
    "            assert weight.shape == partialw[-i].shape\n",
    "            partialw[-i] = weight\n",
    "        return (partialw,partialb)\n",
    "\n",
    "    # Updates our weights and biases with a given batch\n",
    "    def updatebatch(self,batchx,batchy,learningrate = 0.05):\n",
    "        ## Assumptions these are numpy arrays\n",
    "\n",
    "        currentsumb = [np.zeros(b.shape) for b in self.biases]\n",
    "        currentsumw = [np.zeros(w.shape) for w in self.weights]\n",
    "        batchx = batchx.values\n",
    "\n",
    "        for x,y in zip(batchx,batchy):\n",
    "            gradientw,gradientb = self.backprop(x,y)\n",
    "            for i in range(len(currentsumb)):\n",
    "                currentsumb[i] = (gradientb[i] + currentsumb[i])\n",
    "                currentsumw[i] = (gradientw[i] + currentsumw[i])\n",
    "        \n",
    "        for i in range(len(currentsumb)):\n",
    "            self.weights[i] = self.weights[i] - (learningrate/len(batchy)) * currentsumw[i]\n",
    "            self.biases[i] = self.biases[i] - (learningrate/len(batchy)) * currentsumb[i]\n",
    "    \n",
    "\n",
    "            \n",
    "    def train(self,X,Y,batchsize = 32,epoch = 50,testx = None,testy = None):\n",
    "        assert isinstance(X, pd.DataFrame) \n",
    "        assert isinstance(Y,pd.Series)\n",
    "        # Constructs the array for training and test error\n",
    "        self.construct(X,Y)\n",
    "        if testx and testy:\n",
    "            testloss = [self.forward(testx,testy)]\n",
    "        trainingloss = [self.forward(X,Y)]\n",
    "        Y = pd.factorize(Y)[0]\n",
    "        print(trainingloss)\n",
    "        minibatchesX = np.array_split(X, len(X) // batchsize)\n",
    "        minibatchesY = np.array_split(Y, len(Y) // batchsize)\n",
    "        #split dataframe into batches of size batchsize\n",
    "        for i in range(epoch):\n",
    "            for x,y in zip(minibatchesX,minibatchesY):\n",
    "                self.updatebatch(x,y)\n",
    "            \n",
    "            if self.type == 'classification':\n",
    "                print('Your training accuracy after epoch ' + str(i) + ' is ' + str(self.forward(X,Y)))\n",
    "                if testx and testy:\n",
    "                    print('Your test accuracy after epoch ' + str(i) + ' is ' + str(self.forward(testx,testy)))\n",
    "            elif self.type == 'regression':\n",
    "                print('Your training error after epoch ' + str(i) + ' is ' + str(self.forward(X,Y)))\n",
    "                if testx and testy:\n",
    "                    print('Your test error after epoch ' + str(i) + ' is ' + str(self.forward(testx,testy)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "y = y*2 - 1 # make y be -1 or1\n",
    "\n",
    "trainx = pd.DataFrame(X)\n",
    "trainy = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32]\n",
      "52\n",
      "86\n",
      "91\n",
      "96\n",
      "98\n",
      "100\n",
      "99\n",
      "99\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([30,20,30] )\n",
    "nn.train(train_X,train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "     ..\n",
       "95    1\n",
       "96   -1\n",
       "97   -1\n",
       "98   -1\n",
       "99   -1\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
