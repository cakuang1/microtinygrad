{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('house.csv')\n",
    "dataset.drop(['Id'],\n",
    "             axis=1,\n",
    "             inplace=True)\n",
    "\n",
    "dataset['SalePrice'] = dataset['SalePrice'].fillna(\n",
    "  dataset['SalePrice'].mean())\n",
    "\n",
    "\n",
    "new_dataset = dataset.dropna()\n",
    "s = (new_dataset.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "OH_encoder = OneHotEncoder(sparse_output=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(new_dataset[object_cols]))\n",
    "OH_cols.index = new_dataset.index\n",
    "OH_cols.columns = OH_encoder.get_feature_names_out()\n",
    "df_final = new_dataset.drop(object_cols, axis=1)\n",
    "df_final = pd.concat([df_final, OH_cols], axis=1)\n",
    "\n",
    "X = df_final.drop('SalePrice', axis=1)  # Specify the column name for the target variable\n",
    "y = df_final['SalePrice']  # Specify the column name for the target variable\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2356    180921.19589\n",
       "1789    180921.19589\n",
       "2371    180921.19589\n",
       "930     201000.00000\n",
       "2816    180921.19589\n",
       "            ...     \n",
       "1638    180921.19589\n",
       "1095    176432.00000\n",
       "1130    135000.00000\n",
       "1294    115000.00000\n",
       "860     189950.00000\n",
       "Name: SalePrice, Length: 2330, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "def drelu(x):\n",
    "    return np.where(x > 0, 1,0)\n",
    "def mse(t,p):\n",
    "    \"\"\"\n",
    "    Returns mean sqaured error given two arrays of the same length\n",
    "    \"\"\"\n",
    "    return sum((t-p)**2)/len(t)\n",
    "\n",
    "def softmax(x):\n",
    "    # assumes x is a vector\n",
    "\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden = []) -> None:\n",
    "        self.sizes = hidden\n",
    "        self.biases = None\n",
    "        self.weights = None\n",
    "        self.type = None\n",
    "        self.map = {}\n",
    "    # detects what type of model we want and constructs our bias,weights,type and map dependent on the dataframe and series given\n",
    "    def construct(self,X,Y):\n",
    "        inputsize = X.shape[1]\n",
    "        self.sizes.insert(0,inputsize)\n",
    "        # Checking if its categorical or regression NN\n",
    "        if Y.dtype == 'float64':\n",
    "            self.sizes.append(1)\n",
    "            self.type = 'regression'\n",
    "        else:\n",
    "            unique_values = len(Y.unique())\n",
    "            self.sizes.append(unique_values)\n",
    "            self.type = 'classification'\n",
    "            self.map = {index: value for index, value in enumerate(Y.unique())}\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1],self.sizes[1:])]\n",
    "\n",
    "    # Helper for train function\n",
    "    # Feeds our input into our neural network and determines the loss \n",
    "    def forward(self,input,answers):\n",
    "        ## Input is supposed to be the training matrix\n",
    "        input = input.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            dot = np.dot(w, input)+b\n",
    "            if np.array_equal(self.biases[len(self.biases) - 1] ,b):\n",
    "                if self.type == 'classification':\n",
    "                    input = softmax(dot.T)\n",
    "            else:\n",
    "                input = relu(dot)\n",
    "        # input is now the output activation\n",
    "        # If the type is regression, then the length of the output activation should be 1. \n",
    "        if self.type == 'classification':\n",
    "            ## Accuracy instead of loss \n",
    "            currentsum = 0\n",
    "            for row,index in zip(input,answers):\n",
    "                if np.argmax(row) == index:\n",
    "                    currentsum += 1\n",
    "            loss = currentsum\n",
    "        return  loss\n",
    "    \n",
    "    # Returns a tuple of dw,db, which are layer by layer arrays that represent partials\n",
    "    def backprop(self,inputx,inputy):\n",
    "        # input x is a 1d vector ,input y is a scalar\n",
    "        partialb = [np.zeros(b.shape) for b in self.biases]\n",
    "        partialw = [np.zeros(w.shape) for w in self.weights]\n",
    "        storedsums = []\n",
    "        input = inputx.reshape(inputx.size,1)\n",
    "        storedactivations = [input.T]\n",
    "        # Forward propagation, storing the su\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            dot = np.dot(w, input)+b\n",
    "            storedsums.append(dot)\n",
    "            ## If we are on the last iteration, then our activation function is linear if its regression, or softmax if its classification\n",
    "            if np.array_equal(self.biases[len(self.biases) - 1] ,b):\n",
    "                if self.type == 'classification':\n",
    "                    ## Ensure no overflow\n",
    "                    input = softmax(dot.T)\n",
    "            else:\n",
    "                input = relu(dot)\n",
    "            storedactivations.append(input.T)\n",
    "        ## Actual backpropagation\n",
    "        #First Layer\n",
    "        ## Regression case. Thus, the activation for our output for our output layer is linear. Our loss function is mean square error, thus delta (dL/dz) is -2(y-z).\n",
    "        ## Classification case. Delta of the output layer is dependent on the output layer. Credit to https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1 for doing the dirty work\n",
    "        if self.type == 'classification':\n",
    "            dz = storedactivations[-1]\n",
    "            dz[inputy] = dz[inputy] - 1\n",
    "            assert dz.shape == partialb[-1].shape\n",
    "            partialb[-1] = dz\n",
    "            dw = np.dot(dz, storedactivations[-2])\n",
    "            assert dw.shape == partialw[-1].shape\n",
    "            partialw[-1]= dw\n",
    "        \n",
    "        # Finding partials for the rest of the layers\n",
    "        for i in range(2,len(self.sizes)): \n",
    "            dz = drelu(storedsums[-i])\n",
    "\n",
    "            delta = np.dot(self.weights[-i + 1].T,partialb[-i + 1]) * dz\n",
    "            assert delta.shape == partialb[-i].shape\n",
    "            partialb[-i] = delta   \n",
    "            weight = np.dot(delta, storedactivations[-i - 1])\n",
    "            assert weight.shape == partialw[-i].shape\n",
    "            partialw[-i] = weight\n",
    "        return (partialw,partialb)\n",
    "\n",
    "    # Updates our weights and biases with a given batch\n",
    "    def updatebatch(self,batchx,batchy,learningrate = 0.0001):\n",
    "        ## Assumptions these are numpy arrays\n",
    "\n",
    "        currentsumb = [np.zeros(b.shape) for b in self.biases]\n",
    "        currentsumw = [np.zeros(w.shape) for w in self.weights]\n",
    "        batchx = batchx.values\n",
    "\n",
    "        for x,y in zip(batchx,batchy):\n",
    "            gradientw,gradientb = self.backprop(x,y)\n",
    "            for i in range(len(currentsumb)):\n",
    "                currentsumb[i] = (gradientb[i] + currentsumb[i])\n",
    "                currentsumw[i] = (gradientw[i] + currentsumw[i])\n",
    "        \n",
    "        for i in range(len(currentsumb)):\n",
    "            self.weights[i] = self.weights[i] - (learningrate/len(batchy)) * currentsumw[i]\n",
    "            self.biases[i] = self.biases[i] - (learningrate/len(batchy)) * currentsumb[i]\n",
    "    \n",
    "\n",
    "            \n",
    "    def train(self,X,Y,batchsize = 32,epoch = 10,testx = None,testy = None):\n",
    "        assert isinstance(X, pd.DataFrame) \n",
    "        assert isinstance(Y,pd.Series)\n",
    "        # Constructs the array for training and test error\n",
    "        self.construct(X,Y)\n",
    "        if testx and testy:\n",
    "            testloss = [self.forward(testx,testy)]\n",
    "        trainingloss = [self.forward(X,Y)]\n",
    "        print(trainingloss)\n",
    "        if self.type == 'classification':\n",
    "            Y = pd.factorize(Y)[0]\n",
    "        minibatchesX = np.array_split(X, len(X) // batchsize)\n",
    "        minibatchesY = np.array_split(Y, len(Y) // batchsize)\n",
    "        #split dataframe into batches of size batchsize\n",
    "        for i in range(epoch):\n",
    "            for x,y in zip(minibatchesX,minibatchesY):\n",
    "                self.updatebatch(x,y)\n",
    "            \n",
    "            if self.type == 'classification':\n",
    "                print('Your training accuracy after epoch ' + str(i) + ' is ' + str(self.forward(X,Y)))\n",
    "                if testx and testy:\n",
    "                    print('Your test accuracy after epoch ' + str(i) + ' is ' + str(self.forward(testx,testy)))\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or1\n",
    "trainx = pd.DataFrame(X)\n",
    "trainy = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2356    180921.19589\n",
       "1789    180921.19589\n",
       "2371    180921.19589\n",
       "930     201000.00000\n",
       "2816    180921.19589\n",
       "            ...     \n",
       "1638    180921.19589\n",
       "1095    176432.00000\n",
       "1130    135000.00000\n",
       "1294    115000.00000\n",
       "860     189950.00000\n",
       "Name: SalePrice, Length: 2330, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138667173677.29596]\n",
      "180921.19589041095 [[-7945.9507598]]\n",
      "180921.19589041095 [[-178777.09432143]]\n",
      "180921.19589041095 [[-13731.39855112]]\n",
      "201000.0 [[-51389.97162079]]\n",
      "180921.19589041095 [[-213923.16317249]]\n",
      "180921.19589041095 [[-35985.27527704]]\n",
      "153900.0 [[-214973.64330788]]\n",
      "180921.19589041095 [[-66021.09861474]]\n",
      "180921.19589041095 [[-272078.62796439]]\n",
      "212000.0 [[-157323.91725531]]\n",
      "180921.19589041095 [[-150214.7023273]]\n",
      "194500.0 [[-103572.5507115]]\n",
      "180921.19589041095 [[12162.7395652]]\n",
      "190000.0 [[-109967.19438225]]\n",
      "180921.19589041095 [[-93952.49993086]]\n",
      "103200.0 [[6201.89445041]]\n",
      "180921.19589041095 [[-105501.49461884]]\n",
      "173000.0 [[-84278.16904913]]\n",
      "180921.19589041095 [[-168026.69512678]]\n",
      "136000.0 [[-97726.67822035]]\n",
      "180921.19589041095 [[-165388.68073194]]\n",
      "180921.19589041095 [[48438.95112052]]\n",
      "206000.0 [[-7623.41263818]]\n",
      "180921.19589041095 [[-141878.68415531]]\n",
      "180921.19589041095 [[-102239.34206231]]\n",
      "174900.0 [[-81753.06818261]]\n",
      "180921.19589041095 [[-195838.24335076]]\n",
      "154000.0 [[-142078.89760879]]\n",
      "239500.0 [[-104771.22628044]]\n",
      "136500.0 [[37938.76017351]]\n",
      "180921.19589041095 [[-611257.23210766]]\n",
      "155000.0 [[-68489.15124953]]\n",
      "180921.19589041095 [[-110383.55508196]]\n",
      "269500.0 [[1.87740866e+32]]\n",
      "180921.19589041095 [[1.30190622e+32]]\n",
      "180921.19589041095 [[1.64722161e+32]]\n",
      "180921.19589041095 [[1.20966039e+32]]\n",
      "180921.19589041095 [[2.00694407e+32]]\n",
      "318000.0 [[1.89397553e+32]]\n",
      "173500.0 [[1.38083863e+32]]\n",
      "86000.0 [[9.00982064e+31]]\n",
      "180921.19589041095 [[1.73768919e+32]]\n",
      "205000.0 [[1.6819508e+32]]\n",
      "188500.0 [[1.25720959e+32]]\n",
      "108480.0 [[9.067483e+31]]\n",
      "149500.0 [[1.90778274e+32]]\n",
      "142000.0 [[1.10749655e+32]]\n",
      "119750.0 [[1.03776001e+32]]\n",
      "276000.0 [[1.53175525e+32]]\n",
      "109500.0 [[1.43647789e+32]]\n",
      "180921.19589041095 [[9.72284915e+31]]\n",
      "180921.19589041095 [[1.43548819e+32]]\n",
      "132500.0 [[1.25063924e+32]]\n",
      "100000.0 [[1.36536023e+32]]\n",
      "180921.19589041095 [[1.59826978e+32]]\n",
      "180921.19589041095 [[1.41384702e+32]]\n",
      "155900.0 [[7.64667249e+31]]\n",
      "145000.0 [[1.74130029e+32]]\n",
      "180921.19589041095 [[3.52723143e+31]]\n",
      "326000.0 [[2.48475808e+32]]\n",
      "180921.19589041095 [[1.61384627e+32]]\n",
      "180921.19589041095 [[4.22153157e+31]]\n",
      "180921.19589041095 [[1.60560658e+32]]\n",
      "180921.19589041095 [[2.60720418e+32]]\n",
      "180921.19589041095 [[2.01753015e+32]]\n",
      "180921.19589041095 [[1.65836862e+32]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "348000.0 [[-1.62097953e+122]]\n",
      "312500.0 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "139000.0 [[-1.62097953e+122]]\n",
      "140000.0 [[-1.62097953e+122]]\n",
      "144000.0 [[-1.62097953e+122]]\n",
      "315000.0 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "157500.0 [[-1.62097953e+122]]\n",
      "240000.0 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "88000.0 [[-1.62097953e+122]]\n",
      "115000.0 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "271000.0 [[-1.62097953e+122]]\n",
      "127500.0 [[-1.62097953e+122]]\n",
      "105500.0 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "187500.0 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "180921.19589041095 [[-1.62097953e+122]]\n",
      "129500.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "260400.0 [[inf]]\n",
      "130000.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "105900.0 [[inf]]\n",
      "277000.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "196500.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "170000.0 [[inf]]\n",
      "192500.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "171000.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "272000.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "172500.0 [[inf]]\n",
      "254000.0 [[inf]]\n",
      "139000.0 [[inf]]\n",
      "225000.0 [[inf]]\n",
      "150000.0 [[inf]]\n",
      "142953.0 [[inf]]\n",
      "180921.19589041095 [[inf]]\n",
      "208900.0 [[inf]]\n",
      "170000.0 [[inf]]\n",
      "180921.19589041095 [[nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caryk\\AppData\\Local\\Temp\\ipykernel_22420\\1339445669.py:99: RuntimeWarning: invalid value encountered in multiply\n",
      "  delta = np.dot(self.weights[-i + 1].T,partialb[-i + 1]) * dz\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nn \u001b[39m=\u001b[39m NeuralNetwork([\u001b[39m30\u001b[39m,\u001b[39m20\u001b[39m,\u001b[39m30\u001b[39m] )\n\u001b[1;32m----> 2\u001b[0m nn\u001b[39m.\u001b[39;49mtrain(train_X,train_y)\n",
      "Cell \u001b[1;32mIn[74], line 143\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, batchsize, epoch, testx, testy)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m    142\u001b[0m     \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(minibatchesX,minibatchesY):\n\u001b[1;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdatebatch(x,y)\n\u001b[0;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    146\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mYour training accuracy after epoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X,Y)))\n",
      "Cell \u001b[1;32mIn[74], line 116\u001b[0m, in \u001b[0;36mNeuralNetwork.updatebatch\u001b[1;34m(self, batchx, batchy, learningrate)\u001b[0m\n\u001b[0;32m    113\u001b[0m batchx \u001b[39m=\u001b[39m batchx\u001b[39m.\u001b[39mvalues\n\u001b[0;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batchx,batchy):\n\u001b[1;32m--> 116\u001b[0m     gradientw,gradientb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackprop(x,y)\n\u001b[0;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(currentsumb)):\n\u001b[0;32m    118\u001b[0m         currentsumb[i] \u001b[39m=\u001b[39m (gradientb[i] \u001b[39m+\u001b[39m currentsumb[i])\n",
      "Cell \u001b[1;32mIn[74], line 78\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[1;34m(self, inputx, inputy)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[39mprint\u001b[39m(inputy,storedactivations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m---> 78\u001b[0m     \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39misnan(storedactivations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39many() \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     dz \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (inputy \u001b[39m-\u001b[39m storedactivations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     80\u001b[0m     \u001b[39massert\u001b[39;00m dz\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m partialb[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([30,20,30] )\n",
    "nn.train(train_X,train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1    -1\n",
       "2     1\n",
       "3     1\n",
       "4    -1\n",
       "     ..\n",
       "95   -1\n",
       "96   -1\n",
       "97    1\n",
       "98    1\n",
       "99    1\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
