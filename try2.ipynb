{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m s \u001b[39m=\u001b[39m (new_dataset\u001b[39m.\u001b[39mdtypes \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m object_cols \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(s[s]\u001b[39m.\u001b[39mindex)\n\u001b[1;32m---> 13\u001b[0m OH_encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m OH_cols \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(OH_encoder\u001b[39m.\u001b[39mfit_transform(new_dataset[object_cols]))\n\u001b[0;32m     15\u001b[0m OH_cols\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m new_dataset\u001b[39m.\u001b[39mindex\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('house.csv')\n",
    "dataset.drop(['Id'],\n",
    "             axis=1,\n",
    "             inplace=True)\n",
    "\n",
    "dataset['SalePrice'] = dataset['SalePrice'].fillna(\n",
    "  dataset['SalePrice'].mean())\n",
    "\n",
    "\n",
    "new_dataset = dataset.dropna()\n",
    "s = (new_dataset.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "OH_encoder = OneHotEncoder(sparse_output=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(new_dataset[object_cols]))\n",
    "OH_cols.index = new_dataset.index\n",
    "OH_cols.columns = OH_encoder.get_feature_names_out()\n",
    "df_final = new_dataset.drop(object_cols, axis=1)\n",
    "df_final = pd.concat([df_final, OH_cols], axis=1)\n",
    "\n",
    "X = df_final.drop('SalePrice', axis=1)  # Specify the column name for the target variable\n",
    "y = df_final['SalePrice']  # Specify the column name for the target variable\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('house.csv')\n",
    "dataset['BldgType'].dtype == 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy['new'] = pd.factorize(trainy[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {index: value for index, value in enumerate(trainy.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "def drelu(x):\n",
    "    return np.where(x > 0, 1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden = []) -> None:\n",
    "        self.sizes = hidden\n",
    "        self.biases = None\n",
    "        self.weights = None\n",
    "        self.type = None\n",
    "        self.map = {}\n",
    "    # detects what type of model we want and constructs our bias,weights,type and map dependent on the dataframe and series given\n",
    "    def construct(self,X,Y):\n",
    "        inputsize = X.shape[1]\n",
    "        self.sizes.insert(0,inputsize)\n",
    "        # Checking if its categorical or regression NN\n",
    "        if Y.dtype == 'float64':\n",
    "            self.sizes.append(1)\n",
    "            self.type = 'regression'\n",
    "        else:\n",
    "            unique_values = len(Y.unique())\n",
    "            self.sizes.append(unique_values)\n",
    "            self.type = 'classification'\n",
    "            self.map = {index: value for index, value in enumerate(Y.unique())}\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1],self.sizes[1:])]\n",
    "\n",
    "    # Helper for train function\n",
    "    # Feeds our input into our neural network and determines the loss \n",
    "    def forward(self,input,answers):\n",
    "        ## Input is supposed to be the training matrix\n",
    "        input = input.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            print(input.shape)\n",
    "            dot = np.dot(w, input)+b\n",
    "            \n",
    "            input = relu(np.dot(w, input)+b)\n",
    "        # input is now the output activation\n",
    "        # If the type is regression, then the length of the output activation should be 1. \n",
    "        if self.type == 'regression':\n",
    "            loss = ((answers - input) **2)/len(input)\n",
    "        elif self.type == 'classification':\n",
    "            input = input.T\n",
    "            currentsum = 0\n",
    "            for row,index in zip(input,)\n",
    "                \n",
    "            return\n",
    "        return  \n",
    "    \n",
    "    # Returns a tuple of dw,db, which are layer by layer arrays that represent partials\n",
    "    def backprop(self,inputx,inputy):\n",
    "        # input x is a 1d vector ,input y is a scalar\n",
    "        partialb = [np.zeros(b.shape) for b in self.biases]\n",
    "        partialw = [np.zeros(w.shape) for w in self.weights]\n",
    "        storedsums = []\n",
    "        storedactivations = [inputx]\n",
    "        \n",
    "\n",
    "        # Forward propagation, storing the su\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            storedsums.append(z)\n",
    "            activation = relu(z)\n",
    "            storedactivations.append(activation)\n",
    "\n",
    "        ## Actual backpropagation\n",
    "        #First Layer\n",
    "        ## Regression case. Thus, the activation for our output for our output layer is linear. Our loss function is mean square error, thus delta (dL/dz) is -2(y-z).\n",
    "        if self.type == 'regression':\n",
    "            dz = -2 * (y - activation[-1]).reshape(1,1)\n",
    "            assert dz.shape == partialb[-1].shape\n",
    "            dw = np.dot(dz, storedactivations[-2].T)\n",
    "            assert dw.shape == partialw[-1].shape\n",
    "            partialw[-1]= dw\n",
    "        elif self.type == 'classification':\n",
    "            print('test')\n",
    "\n",
    "\n",
    "        # Finding partials for the rest of the layers\n",
    "        for i in range(2,len(self.sizes)): \n",
    "            dz = drelu(storedsums[-i])\n",
    "            delta = np.dot(self.weights[-i + 1],partialb[-i + 1]) * dz\n",
    "            assert delta.shape == partialb[-i].shape\n",
    "            partialb[-i] = delta   \n",
    "            weight = np.dot(delta, storedactivations[-i - 1].T)\n",
    "            assert weight.shape == partialw[-i].shape\n",
    "            partialw[-i] = weight\n",
    "        return (partialw,partialb)\n",
    "\n",
    "    # Updates our weights and biases with a given batch\n",
    "    def updatebatch(self,batchx,batchy,learningrate = 0.05):\n",
    "        ## Assumptions these are numpy arrays\n",
    "        currentsumb = [np.zeros(b.shape) for b in self.biases]\n",
    "        currentsumw = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in zip(batchx,batchy):\n",
    "            gradientw,gradientb = self.backprop(x,y)\n",
    "            for i in range(len(currentsumb)):\n",
    "                currentsumb[i] = (gradientb[i] + currentsumb[i])\n",
    "                currentsumw[i] = (gradientw[i] + currentsumw[i])\n",
    "        \n",
    "        for i in range(len(currentsumb)):\n",
    "            self.weights[i] = self.weights[i] - (learningrate/len(batchy)) * currentsumw[i]\n",
    "            self.weights[i] = self.biases[i] - (learningrate/len(batchy)) * currentsumb[i]\n",
    "            \n",
    "    def train(self,X,Y,batchsize = 32,epoch = 5):\n",
    "        assert isinstance(X, pd.DataFrame) \n",
    "        assert isinstance(Y,pd.Series)\n",
    "        self.construct(X,Y)\n",
    "        \n",
    "        Y = pd.factorize(Y)[0]\n",
    "        self.forward(X,Y)\n",
    "        minibatchesX = np.array_split(X, len(X) // batchsize)\n",
    "        minibatchesY = np.array_split(Y, len(Y) // batchsize)\n",
    "        #split dataframe into batches of size batchsize\n",
    "        for i in range(epoch):\n",
    "            for x,y in zip(minibatchesX,minibatchesY):\n",
    "                print('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "y = y*2 - 1 # make y be -1 or1\n",
    "\n",
    "trainx = pd.DataFrame(X)\n",
    "trainy = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 10, 20, 2]\n",
      "(2, 100)\n",
      "(10, 100)\n",
      "(20, 100)\n",
      "[6.83053443 1.30872693]\n",
      "[41.50068278 10.16084596]\n",
      "[11.01733029  4.73667753]\n",
      "[7.10791862 1.44725316]\n",
      "[16.35469071  3.84433628]\n",
      "[7.44300468 1.51741639]\n",
      "[3.51523865 0.        ]\n",
      "[17.92494857  7.30904948]\n",
      "[27.20288569  9.11834385]\n",
      "[48.43873059 12.43484923]\n",
      "[16.56910051  3.14813534]\n",
      "[11.96817595  5.12922416]\n",
      "[10.23170785  3.87202336]\n",
      "[8.85195705 2.34852841]\n",
      "[51.25356229 12.84857902]\n",
      "[10.64794132  4.53724656]\n",
      "[28.49869247  8.50054641]\n",
      "[22.6529093   9.00649723]\n",
      "[8.42866244 3.9435803 ]\n",
      "[9.18283725 4.01589708]\n",
      "[22.1614716   8.82033009]\n",
      "[31.58556289  6.73450357]\n",
      "[4.18297552 0.        ]\n",
      "[8.81656435 2.33061432]\n",
      "[27.19330726  5.72408964]\n",
      "[13.04026917  2.81894411]\n",
      "[27.71013921  7.42413755]\n",
      "[18.27654894  4.24495194]\n",
      "[29.21679484  7.64066494]\n",
      "[11.68134881  5.01677843]\n",
      "[23.88929569  9.36426039]\n",
      "[25.97567002  8.34761495]\n",
      "[44.66873432 11.00088506]\n",
      "[8.66052851 3.86755067]\n",
      "[7.634255   2.64078046]\n",
      "[12.66356946  5.29607106]\n",
      "[26.93886128  9.39795475]\n",
      "[37.05709667  8.59644591]\n",
      "[13.55243714  5.72975031]\n",
      "[23.12772325  9.11569939]\n",
      "[24.66064062  5.15814937]\n",
      "[3.97591647 0.        ]\n",
      "[4.72628383 1.55718532]\n",
      "[6.68340788 3.48608002]\n",
      "[26.50476766  5.33678142]\n",
      "[14.4402602  6.0609941]\n",
      "[46.93615057 12.01789566]\n",
      "[26.05991701  5.34053066]\n",
      "[50.780342   13.56263518]\n",
      "[22.59876607  8.93024523]\n",
      "[33.53976664  7.24211339]\n",
      "[21.77378644  4.31933861]\n",
      "[46.14460118 11.46551049]\n",
      "[17.8895168   3.63848187]\n",
      "[29.64145375  6.5268896 ]\n",
      "[17.88316611  7.30157298]\n",
      "[27.21836518 10.58828646]\n",
      "[14.52160012  6.09266424]\n",
      "[9.44993651 3.30509488]\n",
      "[10.07322348  3.42502982]\n",
      "[49.73580795 13.96976539]\n",
      "[11.75926176  4.90717325]\n",
      "[9.31247076 2.71045499]\n",
      "[13.21434293  4.87747149]\n",
      "[47.5525063  12.37877268]\n",
      "[5.23318201 0.3903619 ]\n",
      "[3.92509141 0.        ]\n",
      "[15.26139761  3.66298503]\n",
      "[27.02353242  9.20720589]\n",
      "[44.24292751 10.69251446]\n",
      "[18.93894647  3.59000415]\n",
      "[25.92694999  5.17711703]\n",
      "[24.25867278  9.45632704]\n",
      "[5.6039101  0.56029859]\n",
      "[22.83095931  4.5325056 ]\n",
      "[22.54012856  8.95093818]\n",
      "[14.95556665  6.23021129]\n",
      "[24.72182349  9.61989353]\n",
      "[50.21645281 13.18612858]\n",
      "[2.17001328 0.        ]\n",
      "[30.05636346  9.88402781]\n",
      "[10.92605922  4.38141359]\n",
      "[6.45787693 1.14034614]\n",
      "[10.47842179  4.49177113]\n",
      "[52.70362869 14.09048901]\n",
      "[17.12856079  7.06481288]\n",
      "[22.95592005  9.05642271]\n",
      "[35.3332601   7.75498362]\n",
      "[9.16917915 2.23699597]\n",
      "[36.75899265  8.21649911]\n",
      "[4.46838224 0.        ]\n",
      "[33.70889106  7.28850576]\n",
      "[22.04924842  8.79418555]\n",
      "[9.02929885 3.17092384]\n",
      "[28.9961236   8.27355108]\n",
      "[32.2126213   7.03740721]\n",
      "[6.70754215 1.31081976]\n",
      "[23.02483578  9.05350077]\n",
      "[28.19470368 10.64027492]\n",
      "[14.11057259  5.82866645]\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([10,20])\n",
    "nn.train(trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
